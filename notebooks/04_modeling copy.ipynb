{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eaf699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bcdc1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb33d5d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50451efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = joblib.load(\"../data/models/X_train_pre.joblib\")\n",
    "X_val   = joblib.load(\"../data/models/X_val_pre.joblib\")\n",
    "X_test  = joblib.load(\"../data/models/X_test_pre.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49dc86f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train = pd.read_parquet(\"../data/Gold/y_train.parquet\").iloc[:,0].to_numpy()\n",
    "y_val   = pd.read_parquet(\"../data/Gold/y_val.parquet\").iloc[:,0].to_numpy()\n",
    "y_test  = pd.read_parquet(\"../data/Gold/y_test.parquet\").iloc[:,0].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46238b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor  = joblib.load(\"../data/models/preprocessor.joblib\")\n",
    "NUMERIC       = joblib.load(\"../data/models/numeric_cols.joblib\")\n",
    "CATEGORICAL   = joblib.load(\"../data/models/categorical_cols.joblib\")\n",
    "feat_names    = joblib.load(\"../data/models/prepared_feature_names.joblib\")\n",
    "# class_weights = joblib.load(\"../data/models/class_weights.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17afc7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is already in dense array format.\n"
     ]
    }
   ],
   "source": [
    "if hasattr(X_train, \"tocsr\"):\n",
    "    print(\"Converting sparse matrix to dense array for modeling...\")\n",
    "else:\n",
    "    print(\"Data is already in dense array format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9de6841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shapes:\n",
      "  Train: (19139453, 49), Val: (6379894, 49), Test: (6378891, 49)\n",
      "\n",
      " Positive Samples:\n",
      "  Train: 15536, Val: 9054, Test: 10640\n",
      "\n",
      "Class Ratios:\n",
      "  Train: 0.0008, Val: 0.0014, Test: 0.0017\n"
     ]
    }
   ],
   "source": [
    "print(\" Shapes:\")\n",
    "print(f\"  Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "print(\"\\n Positive Samples:\")\n",
    "print(f\"  Train: {int(y_train.sum())}, Val: {int(y_val.sum())}, Test: {int(y_test.sum())}\")\n",
    "\n",
    "print(\"\\nClass Ratios:\")\n",
    "print(f\"  Train: {y_train.mean():.4f}, Val: {y_val.mean():.4f}, Test: {y_test.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48002d27",
   "metadata": {},
   "source": [
    "### Experimental Comparison: Class Weighting vs Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9718cb",
   "metadata": {},
   "source": [
    "Class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70a38ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: 0.9991882735624681, 1: 0.00081172643753194}\n",
      "Scale Pos Weight: 1230.94\n"
     ]
    }
   ],
   "source": [
    "# positives negatives and class weights\n",
    "pos = int(y_train.sum())\n",
    "neg = int((y_train == 0).sum())\n",
    "total = pos + neg\n",
    "\n",
    "class_weights = {\n",
    "    0: neg / total,\n",
    "    1: pos / total\n",
    "}\n",
    "\n",
    "scale_pos_weight = neg / pos if pos > 0 else 1.0\n",
    "\n",
    "print(\"Class Weights:\", class_weights)\n",
    "print(\"Scale Pos Weight:\", round(scale_pos_weight, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0eed33",
   "metadata": {},
   "source": [
    "Evaluation block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4f1ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (roc_auc_score, average_precision_score, precision_recall_fscore_support, precision_recall_curve)\n",
    "\n",
    "\n",
    "def evaluate_block(model_name, y_true, y_prob, thresh=0.5):\n",
    "    y_pred = (y_prob >= thresh).astype(int)\n",
    "    roc = roc_auc_score(y_true, y_prob)\n",
    "    ap  = average_precision_score(y_true, y_prob)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    row = {\n",
    "        \"model\": model_name,\n",
    "        \"roc_auc\": roc,\n",
    "        \"pr_auc\": ap,\n",
    "        \"precision\": p,\n",
    "        \"recall\": r,\n",
    "        \"f1\": f1,\n",
    "        \"threshold\": thresh\n",
    "    }\n",
    "    print(f\"[{model_name}] ROC-AUC:{roc:.4f} | PR-AUC:{ap:.4f} | P:{p:.4f} | R:{r:.4f} | F1:{f1:.4f}\")\n",
    "    return row\n",
    "\n",
    "def save_metrics_table(rows, fname):\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(fname, index=False)\n",
    "    print(f\" Saved metrics â†’ {fname}\")\n",
    "    return df\n",
    "\n",
    "def find_best_threshold(y_true, y_prob):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "    best_idx = f1_scores.argmax()\n",
    "    return thresholds[best_idx], f1_scores[best_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0a8ff2",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c021b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval   = xgb.DMatrix(X_val,   label=y_val)\n",
    "dtest  = xgb.DMatrix(X_test,  label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0541e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-aucpr:0.04379\ttrain-auc:0.96785\tval-aucpr:0.08700\tval-auc:0.97094\n",
      "[50]\ttrain-aucpr:0.34212\ttrain-auc:0.98145\tval-aucpr:0.45113\tval-auc:0.98151\n",
      "[100]\ttrain-aucpr:0.37808\ttrain-auc:0.98410\tval-aucpr:0.48666\tval-auc:0.98284\n",
      "[150]\ttrain-aucpr:0.39018\ttrain-auc:0.98590\tval-aucpr:0.49861\tval-auc:0.98296\n",
      "[180]\ttrain-aucpr:0.39495\ttrain-auc:0.98667\tval-aucpr:0.50199\tval-auc:0.98272\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": [\"aucpr\", \"auc\"],\n",
    "    \"tree_method\": \"hist\",         \n",
    "    \"max_depth\": 6,            \n",
    "    \"eta\": 0.1,                    \n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"min_child_weight\": 20,       \n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "    \"lambda\": 1.0,\n",
    "    \"nthread\": 4                  \n",
    "}\n",
    "\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=1000,          # from 2000\n",
    "    evals=[(dtrain, \"train\"), (dval, \"val\")],\n",
    "    early_stopping_rounds=50,     \n",
    "    verbose_eval=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8334c423",
   "metadata": {},
   "source": [
    "Predict and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7820af58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[XGB [VAL @ 0.5]] ROC-AUC:0.9830 | PR-AUC:0.4945 | P:0.0108 | R:0.9717 | F1:0.0213\n",
      "[XGB [TEST @ 0.5]] ROC-AUC:0.9878 | PR-AUC:0.6345 | P:0.0131 | R:0.9783 | F1:0.0259\n"
     ]
    }
   ],
   "source": [
    "val_prob  = bst.predict(dval,  iteration_range=(0, bst.best_iteration + 1))\n",
    "test_prob = bst.predict(dtest, iteration_range=(0, bst.best_iteration + 1))\n",
    "\n",
    "results = []\n",
    "results.append(evaluate_block(\"XGB [VAL @ 0.5]\",  y_val,  val_prob,  thresh=0.5))\n",
    "results.append({\n",
    "     \"strategy\": \"Class Weighting\",\n",
    "    **evaluate_block(\"XGB [TEST @ 0.5]\", y_test, test_prob, thresh=0.5)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a687ef67",
   "metadata": {},
   "source": [
    "Optimize Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c9cd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.9871 | Best F1: 0.5320\n",
      "[XGB [VAL @ 0.987]] ROC-AUC:0.9830 | PR-AUC:0.4945 | P:0.6801 | R:0.4368 | F1:0.5320\n",
      "[XGB [TEST @ 0.987]] ROC-AUC:0.9878 | PR-AUC:0.6345 | P:0.7847 | R:0.5350 | F1:0.6362\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "thresholds = np.linspace(0.01, 0.99, 99)\n",
    "best_thresh = None\n",
    "best_score = 0\n",
    "\n",
    "for t in thresholds:\n",
    "    preds = (val_prob >= t).astype(int)\n",
    "    precision = precision_score(y_val, preds)\n",
    "    recall = recall_score(y_val, preds)\n",
    "    \n",
    "    # Check if both precision and recall are â‰¥ 90%\n",
    "    if precision >= 0.90 and recall >= 0.90:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        if f1 > best_score:\n",
    "            best_score = f1\n",
    "            best_thresh = t\n",
    "\n",
    "# Report results\n",
    "if best_thresh is not None:\n",
    "    print(f\" Found threshold with Pâ‰¥90% and Râ‰¥90%\")\n",
    "    print(f\"Best threshold: {best_thresh:.4f} | F1: {best_score:.4f}\")\n",
    "    \n",
    "    # Evaluate using this threshold\n",
    "    results.append(evaluate_block(f\"XGB [VAL @ {best_thresh:.3f} | Pâ‰¥90 Râ‰¥90]\", y_val, val_prob, thresh=best_thresh))\n",
    "    results.append(evaluate_block(f\"XGB [TEST @ {best_thresh:.3f} | Pâ‰¥90 Râ‰¥90]\", y_test, test_prob, thresh=best_thresh))\n",
    "else:\n",
    "    print(\"âš ï¸ No threshold found with both precision and recall â‰¥ 90%.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dc1e27",
   "metadata": {},
   "source": [
    "Resampling-SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54feee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train, columns=feat_names)\n",
    "y_train_series = pd.Series(y_train)\n",
    "\n",
    "\n",
    "X_sample = X_train_df.sample(frac=0.6, random_state=42)\n",
    "y_sample = y_train_series.iloc[X_sample.index]\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_sample, y_sample)\n",
    "\n",
    "\n",
    "X_val_df = pd.DataFrame(X_val, columns=feat_names)\n",
    "X_test_df = pd.DataFrame(X_test, columns=feat_names)\n",
    "\n",
    "dtrain_resampled = xgb.DMatrix(X_train_res, label=y_train_res)\n",
    "dval = xgb.DMatrix(X_val_df, label=y_val)\n",
    "dtest = xgb.DMatrix(X_test_df, label=y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d257a343",
   "metadata": {},
   "source": [
    "Predict and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d71ef595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-aucpr:0.97188\ttrain-auc:0.97723\tval-aucpr:0.08867\tval-auc:0.96953\n",
      "[50]\ttrain-aucpr:0.99553\ttrain-auc:0.99588\tval-aucpr:0.39679\tval-auc:0.97755\n",
      "[96]\ttrain-aucpr:0.99884\ttrain-auc:0.99886\tval-aucpr:0.40680\tval-auc:0.97662\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params_resampling = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": [\"aucpr\", \"auc\"],\n",
    "    \"tree_method\": \"hist\",         \n",
    "    \"max_depth\": 6,            \n",
    "    \"eta\": 0.1,                    \n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"min_child_weight\": 20,  \n",
    "    \"lambda\": 1.0,\n",
    "    \"nthread\": 4                  \n",
    "}\n",
    "\n",
    "# Train\n",
    "bst_smote = xgb.train(\n",
    "    params_resampling,\n",
    "    dtrain_resampled,\n",
    "    num_boost_round=500,\n",
    "    evals=[(dtrain_resampled, \"train\"), (dval, \"val\")],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1793b88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Threshold (0.5)\n",
    "results.append(evaluate_block(\"XGB [VAL @ 0.5 | SMOTE]\",  y_val,  val_prob_smote,  thresh=0.5))\n",
    "results.append({\n",
    "     \"strategy\": \"Class Weighting\",\n",
    "    **evaluate_block(\"XGB [TEST @ 0.5 | SMOTE]\", y_test, test_prob_smote, thresh=0.5)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ce0d8c",
   "metadata": {},
   "source": [
    "Optimize Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad226345",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_thresh_smote, best_f1_smote = find_best_threshold(y_val, val_prob_smote)\n",
    "print(f\"Best threshold (SMOTE): {best_thresh_smote:.4f} | Best F1: {best_f1_smote:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acf6bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append(evaluate_block(f\"XGB [VAL @ {best_thresh_smote:.3f} | SMOTE]\", y_val, val_prob_smote, thresh=best_thresh_smote))\n",
    "results.append(evaluate_block(f\"XGB [TEST @ {best_thresh_smote:.3f} | SMOTE]\", y_test, test_prob_smote, thresh=best_thresh_smote))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c182078a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b9d7bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b635891c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352be122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-aucpr:0.08878\ttrain-auc:0.97478\tval-aucpr:0.17054\tval-auc:0.97475\n",
      "[50]\ttrain-aucpr:0.35587\ttrain-auc:0.98501\tval-aucpr:0.46223\tval-auc:0.98192\n",
      "[100]\ttrain-aucpr:0.40968\ttrain-auc:0.98732\tval-aucpr:0.50440\tval-auc:0.98261\n",
      "[150]\ttrain-aucpr:0.42132\ttrain-auc:0.98935\tval-aucpr:0.51185\tval-auc:0.98271\n",
      "[200]\ttrain-aucpr:0.42787\ttrain-auc:0.99083\tval-aucpr:0.51089\tval-auc:0.98203\n",
      "[250]\ttrain-aucpr:0.43697\ttrain-auc:0.99191\tval-aucpr:0.51161\tval-auc:0.98128\n",
      "[254]\ttrain-aucpr:0.43739\ttrain-auc:0.99198\tval-aucpr:0.51166\tval-auc:0.98122\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": [\"aucpr\",\"auc\"],\n",
    "    \"tree_method\": \"hist\",        \n",
    "    \"max_depth\": 8,\n",
    "    \"eta\": 0.08,              \n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"min_child_weight\": 10,\n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "    \"lambda\": 1.0\n",
    "\n",
    "}\n",
    "\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=2000,\n",
    "    evals=[(dtrain,\"train\"), (dval,\"val\")],\n",
    "    early_stopping_rounds=100,\n",
    "    verbose_eval=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabfc3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[XGB [VAL]] ROC-AUC:0.9828 | PR-AUC:0.5124 | P:0.0128 | R:0.9474 | F1:0.0253\n",
      "[XGB [TEST]] ROC-AUC:0.9876 | PR-AUC:0.6466 | P:0.0155 | R:0.9576 | F1:0.0304\n",
      " XGBoost model â†’ ../data/models/xgb_full.json\n"
     ]
    }
   ],
   "source": [
    "# threshold 0.5\n",
    "val_prob  = bst.predict(dval,  iteration_range=(0, bst.best_iteration+1))\n",
    "test_prob = bst.predict(dtest, iteration_range=(0, bst.best_iteration+1))\n",
    "\n",
    "results = []\n",
    "results.append(evaluate_block(\"XGB [VAL]\",  y_val,  val_prob,  thresh=0.5))\n",
    "results.append(evaluate_block(\"XGB [TEST]\", y_test, test_prob, thresh=0.5))\n",
    "\n",
    "joblib.dump(bst, \"../data/models/xgb_model.joblib\")\n",
    "print(\" XGBoost model was as ../data/models/xgb_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95e7aaa",
   "metadata": {},
   "source": [
    "LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa181b40",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() got an unexpected keyword argument 'early_stopping_rounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m      4\u001b[39m val_lgb   = lgb.Dataset(X_val,   label=y_val,   reference=train_lgb, free_raw_data=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      6\u001b[39m params_lgb = {\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mobjective\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetric\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mauc\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33maverage_precision\u001b[39m\u001b[33m\"\u001b[39m],  \u001b[38;5;66;03m# AP ~ PR-AUC\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: -\u001b[32m1\u001b[39m\n\u001b[32m     19\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m gbm = \u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams_lgb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_lgb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_lgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_lgb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mval\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\n\u001b[32m     29\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m val_prob_lgb  = gbm.predict(X_val,  num_iteration=gbm.best_iteration)\n\u001b[32m     32\u001b[39m test_prob_lgb = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
      "\u001b[31mTypeError\u001b[39m: train() got an unexpected keyword argument 'early_stopping_rounds'"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "train_lgb = lgb.Dataset(X_train, label=y_train, free_raw_data=False)\n",
    "val_lgb   = lgb.Dataset(X_val,   label=y_val,   reference=train_lgb, free_raw_data=False)\n",
    "\n",
    "params_lgb = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": [\"auc\",\"average_precision\"],  # AP ~ PR-AUC\n",
    "    \"learning_rate\": 0.08,\n",
    "    \"num_leaves\": 64,\n",
    "    \"max_depth\": -1,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"min_data_in_leaf\": 200,\n",
    "    \"lambda_l2\": 1.0,\n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "    \"verbosity\": -1\n",
    "}\n",
    "\n",
    "gbm = lgb.train(\n",
    "    params_lgb,\n",
    "    train_lgb,\n",
    "    num_boost_round=5000,\n",
    "    valid_sets=[train_lgb, val_lgb],\n",
    "    valid_names=[\"train\",\"val\"],\n",
    "    early_stopping_rounds=200,\n",
    "    verbose_eval=50\n",
    ")\n",
    "\n",
    "val_prob_lgb  = gbm.predict(X_val,  num_iteration=gbm.best_iteration)\n",
    "test_prob_lgb = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "\n",
    "results.append(evaluate_block(\"LGBM [VAL]\",  y_val,  val_prob_lgb))\n",
    "results.append(evaluate_block(\"LGBM [TEST]\", y_test, test_prob_lgb))\n",
    "\n",
    "gbm.save_model(\"../models/lgbm_full.txt\")\n",
    "print(\"ðŸ’¾ Saved LightGBM model â†’ ../models/lgbm_full.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5240b1c",
   "metadata": {},
   "source": [
    "RandomForest (baseline + quick importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90426c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200, max_depth=12, max_features=\"sqrt\",\n",
    "    n_jobs=-1, random_state=42, class_weight=\"balanced_subsample\"\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "val_prob_rf  = rf.predict_proba(X_val)[:,1]\n",
    "test_prob_rf = rf.predict_proba(X_test)[:,1]\n",
    "\n",
    "results.append(evaluate_block(\"RF [VAL]\",  y_val,  val_prob_rf))\n",
    "results.append(evaluate_block(\"RF [TEST]\", y_test, test_prob_rf))\n",
    "\n",
    "# Save RF for reference\n",
    "joblib.dump(rf, \"../models/rf_full.joblib\")\n",
    "print(\"ðŸ’¾ Saved RandomForest â†’ ../models/rf_full.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f7539b",
   "metadata": {},
   "source": [
    "Compare models & pick winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20d3bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = save_metrics_table(results, \"../reports/model_val_test_metrics.csv\")\n",
    "display(tbl.sort_values([\"pr_auc\",\"f1\",\"roc_auc\"], ascending=False).head(10))\n",
    "\n",
    "# Select best by PR-AUC on VAL first, then F1 as tiebreaker\n",
    "val_rows = [r for r in results if \"VAL\" in r[\"model\"]]\n",
    "best_val = sorted(val_rows, key=lambda r: (r[\"pr_auc\"], r[\"f1\"], r[\"roc_auc\"]), reverse=True)[0]\n",
    "best_name = best_val[\"model\"].split(\" \")[0]   # \"XGB\" or \"LGBM\" or \"RF\"\n",
    "best_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069afd8a",
   "metadata": {},
   "source": [
    "Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771ae71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map XGB/LGBM feature indices to names\n",
    "def dump_gain_importance_xgb(bst, feat_names):\n",
    "    imp = bst.get_score(importance_type=\"gain\")  # dict f{idx} -> score\n",
    "    rows = []\n",
    "    for k,v in imp.items():\n",
    "        idx = int(k[1:]) if k.startswith(\"f\") else None\n",
    "        name = feat_names[idx] if (idx is not None and idx < len(feat_names)) else k\n",
    "        rows.append((name, v))\n",
    "    df_imp = pd.DataFrame(rows, columns=[\"feature\",\"gain_importance\"]).sort_values(\"gain_importance\", ascending=False)\n",
    "    df_imp.to_csv(\"../reports/xgb_gain_importance.csv\", index=False)\n",
    "    print(\"ðŸ“„ Saved XGB gain importance â†’ ../reports/xgb_gain_importance.csv\")\n",
    "    return df_imp\n",
    "\n",
    "def dump_gain_importance_lgb(gbm, feat_names):\n",
    "    imp = gbm.feature_importance(importance_type=\"gain\")\n",
    "    df_imp = pd.DataFrame({\"feature\": feat_names[:len(imp)], \"gain_importance\": imp})\n",
    "    df_imp = df_imp.sort_values(\"gain_importance\", ascending=False)\n",
    "    df_imp.to_csv(\"../reports/lgbm_gain_importance.csv\", index=False)\n",
    "    print(\"ðŸ“„ Saved LGBM gain importance â†’ ../reports/lgbm_gain_importance.csv\")\n",
    "    return df_imp\n",
    "\n",
    "if best_name == \"XGB\":\n",
    "    top_imp = dump_gain_importance_xgb(bst, feat_names)\n",
    "elif best_name == \"LGBM\":\n",
    "    top_imp = dump_gain_importance_lgb(gbm, feat_names)\n",
    "else:\n",
    "    # RF impurity-based importance\n",
    "    imp = getattr(rf, \"feature_importances_\", None)\n",
    "    if imp is not None:\n",
    "        df_imp = pd.DataFrame({\"feature\": feat_names[:len(imp)], \"rf_importance\": imp}).sort_values(\"rf_importance\", ascending=False)\n",
    "        df_imp.to_csv(\"../reports/rf_importance.csv\", index=False)\n",
    "        top_imp = df_imp\n",
    "        print(\"ðŸ“„ Saved RF importance â†’ ../reports/rf_importance.csv\")\n",
    "    else:\n",
    "        top_imp = None\n",
    "\n",
    "display(top_imp.head(25) if top_imp is not None else \"No importance table.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9b0a56",
   "metadata": {},
   "source": [
    "Permutation Importance on small VAL subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858df82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "VAL_PI_MAX = 100_000\n",
    "idx = np.random.choice(X_val.shape[0], min(VAL_PI_MAX, X_val.shape[0]), replace=False)\n",
    "\n",
    "# Choose estimator compatible with sklearn interface\n",
    "if best_name == \"XGB\":\n",
    "    # Quick wrapper to use predict_proba-like behavior\n",
    "    # We'll use the trained XGB model's prediction in a lambda\n",
    "    class XGBWrapper:\n",
    "        def __init__(self, booster): self.model = booster\n",
    "        def predict_proba(self, X):\n",
    "            dX = xgb.DMatrix(X)\n",
    "            p = self.model.predict(dX, iteration_range=(0, self.model.best_iteration+1))\n",
    "            return np.c_[1-p, p]\n",
    "    est = XGBWrapper(bst)\n",
    "\n",
    "elif best_name == \"LGBM\":\n",
    "    class LGBWrapper:\n",
    "        def __init__(self, booster): self.model = booster\n",
    "        def predict_proba(self, X):\n",
    "            p = self.model.predict(X, num_iteration=self.model.best_iteration)\n",
    "            return np.c_[1-p, p]\n",
    "    est = LGBWrapper(gbm)\n",
    "\n",
    "else:\n",
    "    est = rf\n",
    "\n",
    "pi = permutation_importance(\n",
    "    est, X_val[idx], y_val[idx],\n",
    "    n_repeats=1, random_state=42,\n",
    "    scoring=\"average_precision\", n_jobs=-1\n",
    ")\n",
    "pi_df = pd.DataFrame({\n",
    "    \"feature\": feat_names[:len(pi.importances_mean)],\n",
    "    \"perm_importance_pr_auc\": np.abs(pi.importances_mean)\n",
    "}).sort_values(\"perm_importance_pr_auc\", ascending=False)\n",
    "\n",
    "pi_df.to_csv(\"../reports/permutation_importance_small_val.csv\", index=False)\n",
    "print(\"ðŸ“„ Saved permutation importance (small VAL) â†’ ../reports/permutation_importance_small_val.csv\")\n",
    "display(pi_df.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de84c5ff",
   "metadata": {},
   "source": [
    "SMOTE experiment on a tiny train subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621c61e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only to demonstrate resampling; do NOT run on full 31M (too heavy).\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "N_SUB = 300_000  # small subset for demo\n",
    "# stratified sample\n",
    "pos_idx = np.where(y_train == 1)[0]\n",
    "neg_idx = np.where(y_train == 0)[0]\n",
    "\n",
    "n_pos = len(pos_idx)\n",
    "n_neg_target = max(n_pos * 100, 1_000)  # cap majority for speed\n",
    "sel_neg = np.random.choice(neg_idx, size=min(n_neg_target, len(neg_idx)), replace=False)\n",
    "sub_idx = np.r_[pos_idx, sel_neg]\n",
    "np.random.shuffle(sub_idx)\n",
    "\n",
    "X_sub = X_train[sub_idx]\n",
    "y_sub = y_train[sub_idx]\n",
    "\n",
    "sm = SMOTE(random_state=42, n_jobs=-1)\n",
    "X_sm, y_sm = sm.fit_resample(X_sub, y_sub)\n",
    "\n",
    "# A quick model on resampled data (LogReg saga)\n",
    "lr_sm = LogisticRegression(\n",
    "    solver=\"saga\", max_iter=200, tol=1e-3, n_jobs=-1, C=0.5\n",
    ")\n",
    "lr_sm.fit(X_sm, y_sm)\n",
    "\n",
    "val_prob_sm = lr_sm.predict_proba(X_val)[:,1]\n",
    "_ = evaluate_block(\"SMOTE(LogReg) [VAL]\", y_val, val_prob_sm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e1fed6",
   "metadata": {},
   "source": [
    "Save final â€œwinnerâ€ predictions & thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab1b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick winner by highest PR-AUC on VAL among XGB/LGBM/RF\n",
    "summary = pd.read_csv(\"../reports/model_val_test_metrics.csv\")\n",
    "summary.sort_values([\"pr_auc\",\"f1\",\"roc_auc\"], ascending=False, inplace=True)\n",
    "display(summary.head(6))\n",
    "\n",
    "winner = summary.iloc[0][\"model\"]\n",
    "if winner.startswith(\"XGB\"):\n",
    "    final_val_prob  = val_prob\n",
    "    final_test_prob = test_prob\n",
    "    chosen = \"xgb_full.json\"\n",
    "elif winner.startswith(\"LGBM\"):\n",
    "    final_val_prob  = val_prob_lgb\n",
    "    final_test_prob = test_prob_lgb\n",
    "    chosen = \"lgbm_full.txt\"\n",
    "else:\n",
    "    final_val_prob  = val_prob_rf\n",
    "    final_test_prob = test_prob_rf\n",
    "    chosen = \"rf_full.joblib\"\n",
    "\n",
    "# Optional: tune threshold to maximize F1 on VAL\n",
    "from sklearn.metrics import f1_score\n",
    "thr_grid = np.linspace(0.01, 0.50, 50)\n",
    "f1s = [f1_score(y_val, (final_val_prob>=t).astype(int)) for t in thr_grid]\n",
    "best_thr = float(thr_grid[int(np.argmax(f1s))])\n",
    "\n",
    "final_row_val  = evaluate_block(f\"WINNER({winner}) [VAL tuned]\",  y_val,  final_val_prob,  best_thr)\n",
    "final_row_test = evaluate_block(f\"WINNER({winner}) [TEST tuned]\", y_test, final_test_prob, best_thr)\n",
    "\n",
    "pd.DataFrame([final_row_val, final_row_test]).to_csv(\"../reports/winner_tuned_metrics.csv\", index=False)\n",
    "json.dump({\"winner\": winner, \"model_file\": chosen, \"best_threshold\": best_thr},\n",
    "          open(\"../models/winner_meta.json\",\"w\"))\n",
    "print(f\"ðŸ Winner: {winner} | Model file: {chosen} | Best threshold (VAL): {best_thr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578b86ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "thresholds = np.linspace(0.01, 0.99, 99)\n",
    "high_score = []\n",
    "\n",
    "for t in thresholds:\n",
    "    preds = (val_prob_weighted >= t).astype(int)\n",
    "    precision = precision_score(y_val, preds)\n",
    "    recall = recall_score(y_val, preds)\n",
    "    \n",
    "    if precision >= 0.90 and recall >= 0.90:\n",
    "        high_score.append((t, precision, recall))\n",
    "\n",
    "# Show results\n",
    "if high_score:\n",
    "    print(\"Thresholds with both precision and recall â‰¥ 90%:\")\n",
    "    for t, p, r in high_score:\n",
    "        print(f\"Threshold: {t:.4f} | Precision: {p:.4f} | Recall: {r:.4f}\")\n",
    "else:\n",
    "    print(\"No threshold found with both precision and recall â‰¥ 90%.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
